---
title: kafka
type: output
categories: ["Services"]
---

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the contents of:
     lib/output/kafka.go
-->

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';


The kafka output type writes a batch of messages to Kafka brokers and waits for
acknowledgement before propagating it back to the input.


<Tabs defaultValue="common" values={[
  { label: 'Common', value: 'common', },
  { label: 'Advanced', value: 'advanced', },
]}>

<TabItem value="common">

```yaml
# Common config fields, showing default values
output:
  kafka:
    addresses:
      - localhost:9092
    topic: benthos_stream
    client_id: benthos_kafka_output
    key: ""
    partitioner: fnv1a_hash
    compression: none
    static_headers: {}
    max_in_flight: 1
    batching:
      count: 0
      byte_size: 0
      period: ""
      check: ""
```

</TabItem>
<TabItem value="advanced">

```yaml
# All config fields, showing default values
output:
  kafka:
    addresses:
      - localhost:9092
    tls:
      enabled: false
      skip_cert_verify: false
      root_cas_file: ""
      client_certs: []
    sasl:
      mechanism: ""
      user: ""
      password: ""
      access_token: ""
      token_cache: ""
      token_key: ""
    topic: benthos_stream
    client_id: benthos_kafka_output
    key: ""
    partitioner: fnv1a_hash
    compression: none
    static_headers: {}
    max_in_flight: 1
    ack_replicas: false
    max_msg_bytes: 1000000
    timeout: 5s
    target_version: 1.0.0
    batching:
      count: 0
      byte_size: 0
      period: ""
      check: ""
      processors: []
    max_retries: 0
    backoff:
      initial_interval: 3s
      max_interval: 10s
      max_elapsed_time: 30s
```

</TabItem>
</Tabs>

The config field `ack_replicas` determines whether we wait for
acknowledgement from all replicas or just a single broker.

Both the `key` and `topic` fields can be dynamically set using
function interpolations described [here](/docs/configuration/interpolation#bloblang-queries).
When sending batched messages these interpolations are performed per message
part.

## Performance

This output benefits from sending multiple messages in flight in parallel for
improved performance. You can tune the max number of in flight messages with the
field `max_in_flight`.

This output benefits from sending messages as a batch for improved performance.
Batches can be formed at both the input and output level. You can find out more
[in this doc](/docs/configuration/batching).

## Fields

### `addresses`

A list of broker addresses to connect to. If an item of the list contains commas it will be expanded into multiple addresses.


Type: `array`  
Default: `["localhost:9092"]`  

```yaml
# Examples

addresses:
  - localhost:9092

addresses:
  - localhost:9041,localhost:9042

addresses:
  - localhost:9041
  - localhost:9042
```

### `tls`

Custom TLS settings can be used to override system defaults.


Type: `object`  

### `tls.enabled`

Whether custom TLS settings are enabled.


Type: `bool`  
Default: `false`  

### `tls.skip_cert_verify`

Whether to skip server side certificate verification.


Type: `bool`  
Default: `false`  

### `tls.root_cas_file`

The path of a root certificate authority file to use.


Type: `string`  
Default: `""`  

### `tls.client_certs`

A list of client certificates to use. For each certificate either the fields `cert` and `key`, or `cert_file` and `key_file` should be specified, but not both.


Type: `array`  

```yaml
# Examples

client_certs:
  - cert: foo
    key: bar

client_certs:
  - cert_file: ./example.pem
    key_file: ./example.key
```

### `tls.client_certs[].cert`

A plain text certificate to use.


Type: `string`  
Default: `""`  

### `tls.client_certs[].key`

A plain text certificate key to use.


Type: `string`  
Default: `""`  

### `tls.client_certs[].cert_file`

The path to a certificate to use.


Type: `string`  
Default: `""`  

### `tls.client_certs[].key_file`

The path of a certificate key to use.


Type: `string`  
Default: `""`  

### `sasl`

Enables SASL authentication.


Type: `object`  

### `sasl.mechanism`

The SASL authentication mechanism, if left empty SASL authentication is not used. Warning: SCRAM based methods within Benthos have not received a security audit.


Type: `string`  
Default: `""`  
Options: `PLAIN`, `OAUTHBEARER`, `SCRAM-SHA-256`, `SCRAM-SHA-512`.

### `sasl.user`

A `PLAIN` username. It is recommended that you use environment variables to populate this field.


Type: `string`  
Default: `""`  

```yaml
# Examples

user: ${USER}
```

### `sasl.password`

A `PLAIN` password. It is recommended that you use environment variables to populate this field.


Type: `string`  
Default: `""`  

```yaml
# Examples

password: ${PASSWORD}
```

### `sasl.access_token`

A static `OAUTHBEARER` access token


Type: `string`  
Default: `""`  

### `sasl.token_cache`

Instead of using a static `access_token` allows you to query a [`cache`](/docs/components/caches/about) resource to fetch `OAUTHBEARER` tokens from


Type: `string`  
Default: `""`  

### `sasl.token_key`

Required when using a `token_cache`, the key to query the cache with for tokens.


Type: `string`  
Default: `""`  

### `topic`

The topic to publish messages to.
This field supports [interpolation functions](/docs/configuration/interpolation#bloblang-queries).


Type: `string`  
Default: `"benthos_stream"`  

### `client_id`

An identifier for the client connection.


Type: `string`  
Default: `"benthos_kafka_output"`  

### `key`

The key to publish messages with.
This field supports [interpolation functions](/docs/configuration/interpolation#bloblang-queries).


Type: `string`  
Default: `""`  

### `partitioner`

The partitioning algorithm to use.


Type: `string`  
Default: `"fnv1a_hash"`  
Options: `fnv1a_hash`, `murmur2_hash`, `random`, `round_robin`.

### `compression`

The compression algorithm to use.


Type: `string`  
Default: `"none"`  
Options: `none`, `snappy`, `lz4`, `gzip`.

### `static_headers`

An optional map of static headers that should be added to messages in addition to metadata.


Type: `object`  
Default: `{}`  

```yaml
# Examples

static_headers:
  first-static-header: value-1
  second-static-header: value-2
```

### `max_in_flight`

The maximum number of parallel message batches to have in flight at any given time.


Type: `number`  
Default: `1`  

### `ack_replicas`

Ensure that messages have been copied across all replicas before acknowledging receipt.


Type: `bool`  
Default: `false`  

### `max_msg_bytes`

The maximum size in bytes of messages sent to the target topic.


Type: `number`  
Default: `1000000`  

### `timeout`

The maximum period of time to wait for message sends before abandoning the request and retrying.


Type: `string`  
Default: `"5s"`  

### `target_version`

The version of the Kafka protocol to use.


Type: `string`  
Default: `"1.0.0"`  

### `batching`

Allows you to configure a [batching policy](/docs/configuration/batching).


Type: `object`  

```yaml
# Examples

batching:
  byte_size: 5000
  count: 0
  period: 1s

batching:
  count: 10
  period: 1s

batching:
  check: this.contains("END BATCH")
  count: 0
  period: 1m
```

### `batching.count`

A number of messages at which the batch should be flushed. If `0` disables count based batching.


Type: `number`  
Default: `0`  

### `batching.byte_size`

An amount of bytes at which the batch should be flushed. If `0` disables size based batching.


Type: `number`  
Default: `0`  

### `batching.period`

A period in which an incomplete batch should be flushed regardless of its size.


Type: `string`  
Default: `""`  

```yaml
# Examples

period: 1s

period: 1m

period: 500ms
```

### `batching.check`

A [Bloblang query](/docs/guides/bloblang/about/) that should return a boolean value indicating whether a message should end a batch.


Type: `string`  
Default: `""`  

```yaml
# Examples

check: this.type == "end_of_transaction"
```

### `batching.processors`

A list of [processors](/docs/components/processors/about) to apply to a batch as it is flushed. This allows you to aggregate and archive the batch however you see fit. Please note that all resulting messages are flushed as a single batch, therefore splitting the batch into smaller batches using these processors is a no-op.


Type: `array`  
Default: `[]`  

```yaml
# Examples

processors:
  - archive:
      format: lines

processors:
  - archive:
      format: json_array

processors:
  - merge_json: {}
```

### `max_retries`

The maximum number of retries before giving up on the request. If set to zero there is no discrete limit.


Type: `number`  
Default: `0`  

### `backoff`

Control time intervals between retry attempts.


Type: `object`  

### `backoff.initial_interval`

The initial period to wait between retry attempts.


Type: `string`  
Default: `"3s"`  

### `backoff.max_interval`

The maximum period to wait between retry attempts.


Type: `string`  
Default: `"10s"`  

### `backoff.max_elapsed_time`

The maximum period to wait before retry attempts are abandoned. If zero then no limit is used.


Type: `string`  
Default: `"30s"`  


